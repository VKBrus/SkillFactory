kubectl cluster-info
Kubernetes control plane is running at https://172.17.0.1:6443
CoreDNS is running at https://172.17.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy



bob@master:~$ kubectl get nodes -o wide
NAME     STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
master   Ready    control-plane,master   28m   v1.22.2   10.244.0.3    <none>        Ubuntu 16.04.7 LTS   4.4.0-210-generic   docker://20.10.7
bob@master:~$ 

bob@master:~$ kubectl get po -A
NAMESPACE     NAME                             READY   STATUS    RESTARTS      AGE
kube-system   coredns-78fcd69978-cx5tm         1/1     Running   0             29m
kube-system   coredns-78fcd69978-m6fgh         1/1     Running   0             29m
kube-system   etcd-master                      1/1     Running   0             29m
kube-system   kube-apiserver-master            1/1     Running   0             29m
kube-system   kube-controller-manager-master   1/1     Running   0             29m
kube-system   kube-proxy-fk6cf                 1/1     Running   0             29m
kube-system   kube-scheduler-master            1/1     Running   0             29m
kube-system   weave-net-mmk9p                  2/2     Running   1 (22m ago)   23m
bob@master:~$ 

bob@master:~$ kubectl get events -A
NAMESPACE     LAST SEEN   TYPE      REASON                    OBJECT                               MESSAGE
default       33m         Normal    Starting                  node/master                          Starting kubelet.
default       33m         Normal    NodeHasSufficientMemory   node/master                          Node master status is now: NodeHasSufficientMemory
default       33m         Normal    NodeHasNoDiskPressure     node/master                          Node master status is now: NodeHasNoDiskPressure
default       33m         Normal    NodeHasSufficientPID      node/master                          Node master status is now: NodeHasSufficientPID
default       33m         Normal    NodeAllocatableEnforced   node/master                          Updated Node Allocatable limit across pods
default       33m         Normal    RegisteredNode            node/master                          Node master event: Registered Node master in Controller
default       27m         Normal    NodeReady                 node/master                          Node master status is now: NodeReady
kube-system   27m         Warning   FailedScheduling          pod/coredns-78fcd69978-cx5tm         0/1 nodes are available: 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn't tolerate.
kube-system   26m         Normal    Scheduled                 pod/coredns-78fcd69978-cx5tm         Successfully assigned kube-system/coredns-78fcd69978-cx5tm to master
kube-system   26m         Normal    Pulled                    pod/coredns-78fcd69978-cx5tm         Container image "k8s.gcr.io/coredns/coredns:v1.8.4" already present on machine
kube-system   26m         Normal    Created                   pod/coredns-78fcd69978-cx5tm         Created container coredns
kube-system   26m         Normal    Started                   pod/coredns-78fcd69978-cx5tm         Started container coredns
kube-system   27m         Warning   FailedScheduling          pod/coredns-78fcd69978-m6fgh         0/1 nodes are available: 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn't tolerate.
kube-system   26m         Normal    Scheduled                 pod/coredns-78fcd69978-m6fgh         Successfully assigned kube-system/coredns-78fcd69978-m6fgh to master
kube-system   26m         Normal    Pulled                    pod/coredns-78fcd69978-m6fgh         Container image "k8s.gcr.io/coredns/coredns:v1.8.4" already present on machine
kube-system   26m         Normal    Created                   pod/coredns-78fcd69978-m6fgh         Created container coredns
kube-system   26m         Normal    Started                   pod/coredns-78fcd69978-m6fgh         Started container coredns
kube-system   33m         Normal    SuccessfulCreate          replicaset/coredns-78fcd69978        Created pod: coredns-78fcd69978-m6fgh
kube-system   33m         Normal    SuccessfulCreate          replicaset/coredns-78fcd69978        Created pod: coredns-78fcd69978-cx5tm
kube-system   33m         Normal    ScalingReplicaSet         deployment/coredns                   Scaled up replica set coredns-78fcd69978 to 2
kube-system   33m         Warning   NodeNotReady              pod/etcd-master                      Node is not ready
kube-system   33m         Warning   NodeNotReady              pod/kube-apiserver-master            Node is not ready
kube-system   33m         Warning   NodeNotReady              pod/kube-controller-manager-master   Node is not ready
kube-system   33m         Normal    LeaderElection            lease/kube-controller-manager        master_68d9effe-7250-4245-820a-32f50c5a5670 became leader
kube-system   33m         Normal    Scheduled                 pod/kube-proxy-fk6cf                 Successfully assigned kube-system/kube-proxy-fk6cf to master
kube-system   33m         Warning   FailedMount               pod/kube-proxy-fk6cf                 MountVolume.SetUp failed for volume "kube-api-access-2vzwk" : configmap "kube-root-ca.crt" not found
kube-system   33m         Normal    Pulled                    pod/kube-proxy-fk6cf                 Container image "k8s.gcr.io/kube-proxy:v1.22.2" already present on machine
kube-system   33m         Normal    Created                   pod/kube-proxy-fk6cf                 Created container kube-proxy
kube-system   33m         Normal    Started                   pod/kube-proxy-fk6cf                 Started container kube-proxy
kube-system   33m         Normal    SuccessfulCreate          daemonset/kube-proxy                 Created pod: kube-proxy-fk6cf
kube-system   33m         Warning   NodeNotReady              pod/kube-scheduler-master            Node is not ready
kube-system   33m         Normal    LeaderElection            lease/kube-scheduler                 master_ccd7975d-a963-48ec-bdb9-f9d77c90ded1 became leader
kube-system   27m         Normal    Scheduled                 pod/weave-net-mmk9p                  Successfully assigned kube-system/weave-net-mmk9p to master
kube-system   27m         Normal    Pulling                   pod/weave-net-mmk9p                  Pulling image "docker.io/weaveworks/weave-kube:2.8.1"
kube-system   27m         Normal    Pulled                    pod/weave-net-mmk9p                  Successfully pulled image "docker.io/weaveworks/weave-kube:2.8.1" in 5.479337881s
kube-system   27m         Normal    Created                   pod/weave-net-mmk9p                  Created container weave-init
kube-system   27m         Normal    Started                   pod/weave-net-mmk9p                  Started container weave-init
kube-system   27m         Normal    Pulled                    pod/weave-net-mmk9p                  Container image "docker.io/weaveworks/weave-kube:2.8.1" already present on machine
kube-system   27m         Normal    Created                   pod/weave-net-mmk9p                  Created container weave
kube-system   27m         Normal    Started                   pod/weave-net-mmk9p                  Started container weave
kube-system   27m         Normal    Pulling                   pod/weave-net-mmk9p                  Pulling image "docker.io/weaveworks/weave-npc:2.8.1"
kube-system   27m         Normal    Pulled                    pod/weave-net-mmk9p                  Successfully pulled image "docker.io/weaveworks/weave-npc:2.8.1" in 4.474672075s
kube-system   27m         Normal    Created                   pod/weave-net-mmk9p                  Created container weave-npc
kube-system   27m         Normal    Started                   pod/weave-net-mmk9p                  Started container weave-npc
kube-system   27m         Normal    SuccessfulCreate          daemonset/weave-net                  Created pod: weave-net-mmk9p
bob@master:~$ 



===============================

bob@worker2:~$ sudo kubeadm join 172.17.0.1:6443 --token 6mid0v.br7zdb08jrvjzkmc --discovery-token-ca-cert-hash sha256:f270599d60449d08a0ba3701906a0e0b4af88cbc91dca6405369e37587166c52
[preflight] Running pre-flight checks
error execution phase preflight: couldn't validate the identity of the API Server: Get "https://172.17.0.1:6443/api/v1/namespaces/kube-public/configmaps/cluster-info?timeout=10s": dial tcp 172.17.0.1:6443: connect: connection refused
To see the stack trace of this error execute with --v=5 or higher

Если же внешний адрес 

bob@worker2:~$ sudo kubeadm join 130.193.58.32:6443 --token 6mid0v.br7zdb08jrvjzkmc --discovery-token-ca-cert-hash sha256:f270599d60449d08a0ba3701906a0e0b4af88cbc91dca6405369e37587166c52
[preflight] Running pre-flight checks
error execution phase preflight: couldn't validate the identity of the API Server: Get "https://130.193.58.32:6443/api/v1/namespaces/kube-public/configmaps/cluster-info?timeout=10s": x509: certificate is valid for 10.96.0.1, 172.17.0.1, not 130.193.58.32
To see the stack trace of this error execute with --v=5 or higher





